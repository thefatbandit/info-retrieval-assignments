{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda1c5afe4909b94d1f9ad8012633fc5cb5",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Task 2\n",
    "\n",
    "### Task 2.1\n",
    "Consists of extracting data from the dictionary and adding the same in a nested dictionary & storing them as JSON files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os \n",
    "import re\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ECTNestedDict = {}\n",
    "transcript = {}\n",
    "file_path = os.getcwd() + \"/data/\"\n",
    "\n",
    "script_file_path = os.getcwd() + \"/ECTNestedDict/\"\n",
    "os.mkdir(script_file_path)\n",
    "file_list = os.listdir(file_path)\n",
    "debug_list = []\n",
    "\n",
    "# Dictionary Construction\n",
    "for i in range(len(file_list)):\n",
    "    path = file_path + file_list[i]\n",
    "    ID = re.findall(r'\\d+', file_list[i])[0]\n",
    "    # print(\"DocID: \" + ID)\n",
    "\n",
    "    tr_file = open(path)\n",
    "    # Parsing the html\n",
    "    soup = BeautifulSoup(tr_file, features=\"html.parser\")\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------\n",
    "    # Finding & Storing Date\n",
    "    # Storing the 1st line\n",
    "    a = (soup.find('p', {'class' : 'p p1'})).get_text()  # For finding a specific class in the html\n",
    "    #  Finding the date using regex\n",
    "    a = re.findall(r'[A-Z]\\w+\\s+\\d+\\,\\s+\\d+',a)\n",
    "\n",
    "    \n",
    "    # Brute-Forcing the \"p p_1\" class in case Dat not in heading\n",
    "    if(not a): \n",
    "        temp = soup.findAll('p', class_=\"p p1\")\n",
    "        for j in range(5): # Checking in the 1st 5 tags\n",
    "            a = re.findall(r'[A-Z]\\w+\\s+\\d+\\,\\s+\\d+', temp[j].get_text())\n",
    "            if(not a):\n",
    "                a = re.findall(r'[A-Z]\\w+\\s+\\d+\\s+\\d+', temp[j].get_text())\n",
    "            if(a):\n",
    "                break\n",
    "    if(not a):\n",
    "        print(temp[j].get_text())\n",
    "    # Removing extra spaces from the date\n",
    "    date = \" \".join(a[0].split())\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------\n",
    "    # Constructing Participant List\n",
    "    a = soup.findAll('p')\n",
    "    part_list= []\n",
    "    flag=0  # flag for storing no. of strong texts <strong> texts\n",
    "    ctr=-1\n",
    "\n",
    "    while(flag<3):\n",
    "        ctr+=1\n",
    "        tag = a[ctr]\n",
    "        if(tag.find('strong')):\n",
    "            flag+=1\n",
    "        elif(flag>0):\n",
    "            part_list.append(tag.get_text())\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------\n",
    "    # Constructing Presentation Dictionary\n",
    "    presentation_dict ={}\n",
    "    temp =[]\n",
    "    speaker = \"\"\n",
    "    flag = 0\n",
    "    qna_head_phrases = [\"question-and-answer session\", \"question-and-answers session\", \"questions-and-answers session\", \"questions-and-answer session\"]\n",
    "\n",
    "    while(ctr<len(a) and (\" \".join(((a[ctr].get_text()).lower()).split()) not in qna_head_phrases)):\n",
    "        tag = a[ctr]\n",
    "        if(tag.find('strong')):\n",
    "            if((tag.get_text() not in presentation_dict.keys())):\n",
    "                if(flag==0):\n",
    "                    speaker = tag.get_text()\n",
    "                    flag=1\n",
    "                elif(flag!=0):\n",
    "                    presentation_dict[speaker] = temp\n",
    "                    temp=[]\n",
    "                    speaker = tag.get_text()\n",
    "            else:\n",
    "                presentation_dict[speaker] = temp\n",
    "                speaker = tag.get_text()\n",
    "                temp = presentation_dict[speaker]\n",
    "        else:\n",
    "            temp.append(tag.get_text())\n",
    "        ctr+=1\n",
    "    else: # To add the last speaker to presentation_dict\n",
    "        presentation_dict[speaker] = temp\n",
    "    \n",
    "    if(ctr==len(a)):\n",
    "        debug_list.append(ID)\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------\n",
    "    # Construction of Questionnaire Dictionary\n",
    "    ctr+=1 # To move forward from the Q-&-A Heading tag\n",
    "    question_dict = {}\n",
    "    temp = {}\n",
    "    speaker = \"\"\n",
    "    flag = 0\n",
    "    remarks = []\n",
    "    flow_ctr= 1\n",
    "    while(ctr<len(a)): # Looping till end of the document\n",
    "        tag =a[ctr]\n",
    "        if((tag.find('strong') or (tag.get_text() in part_list))):\n",
    "            if(flag==0):\n",
    "                speaker = tag.get_text()\n",
    "                flag=1\n",
    "            else:\n",
    "                temp[\"Speaker\"] = deepcopy(speaker)\n",
    "                temp[\"Remarks\"] = remarks\n",
    "                question_dict[flow_ctr] = deepcopy(temp) # Important to use deepcopy\n",
    "                speaker = tag.get_text()\n",
    "                remarks = []\n",
    "                temp = {}\n",
    "                flow_ctr+=1\n",
    "        else:\n",
    "            remarks.append(tag.get_text())\n",
    "        ctr+=1\n",
    "    else: # To add the last speaker to question_dict. \n",
    "        temp[\"Speaker\"] = deepcopy(speaker)\n",
    "        temp[\"Remarks\"] = remarks\n",
    "        question_dict[flow_ctr] = deepcopy(temp)\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------    \n",
    "    # Creating the Transcript Instance \n",
    "    transcript[\"Date\"] = date\n",
    "    transcript[\"Presentation\"] = presentation_dict\n",
    "    transcript[\"Questionnaire\"] = question_dict\n",
    "    # Adding the transcript to overall nested dictionary\n",
    "    ECTNestedDict[\"Transcript-\" + str(i+1)] = deepcopy(transcript)\n",
    "\n",
    "    #  --------------------------------------------------------------------------------------------------------------    \n",
    "    # Saving the transcript Dictionary in a JSON file\n",
    "    script_file_path = os.getcwd() + \"/ECTNestedDict/\"\n",
    "    json_name = \"transcript-\" + str(i+1) + \".json\"\n",
    "    with open(script_file_path + json_name, 'w') as fp:\n",
    "        json.dump(transcript, fp, sort_keys=True, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the entire transcripts in a big JSON.\n",
    "script_file_path = os.getcwd() + \"/\"\n",
    "json_name = \"ECTNestedDict.json\"\n",
    "with open(script_file_path + json_name, 'w') as fp:\n",
    "    json.dump(ECTNestedDict, fp, sort_keys=True, indent=3)"
   ]
  },
  {
   "source": [
    "### Task 2.2\n",
    "Extracting only the text files from **ECTNestedDict** & store in **ECTText**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JSON file location\n",
    "file_path = os.getcwd() + \"/ECTNestedDict/\"\n",
    "# Creating \"ECTText\" folder\n",
    "txt_file_path = os.getcwd() + \"/ECTText/\"\n",
    "os.mkdir(txt_file_path)\n",
    "file_list = os.listdir(file_path)\n",
    "debug_list = []\n",
    "\n",
    "for i in range(1,len(file_list)+1):\n",
    "    # JSON + TXT file name for Read/Write \n",
    "    json_file_name = file_path + \"transcript-\" + str(i) + \".json\"\n",
    "    txt_file_name = txt_file_path + \"transcript-\" + str(i) + \".txt\"\n",
    "    text = \"\"\n",
    "\n",
    "    # Opening JSON file \n",
    "    with open(json_file_name, 'r') as transcript_file:\n",
    "        transcript = json.load(transcript_file)\n",
    "        presentation_dict = transcript[\"Presentation\"]\n",
    "        for presenter in presentation_dict:\n",
    "            remarks = presentation_dict[presenter]\n",
    "            if(remarks):\n",
    "                text = text + remarks[0]\n",
    "            for j in range(1, len(remarks)):\n",
    "                text = text + \" \" + remarks[j]\n",
    "        question_dict = transcript[\"Questionnaire\"]\n",
    "        for speaker in question_dict:\n",
    "            remarks = question_dict[speaker][\"Remarks\"]\n",
    "            for temp in remarks:\n",
    "                text = text + \" \" + temp\n",
    "    with open(txt_file_name,\"wt\") as text_file:\n",
    "        text_file.write(text)\n"
   ]
  },
  {
   "source": [
    "## Task 3\n",
    "\n",
    "### Task 3.1\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Stop-Words/Punctuations Removal\n",
    "\n",
    "\n",
    "### Task 3.2\n",
    "- Build Inverted Positional Index\n",
    "\n",
    "#### **HINT-** Positional Index Structure\n",
    "\n",
    "![Example Posting Index](img/1.png)\n",
    "\n",
    "For the current example this what our Posting-List structure would look like:\n",
    "\n",
    "```\n",
    "{\"hello\" : { 3 : [120, 125, 278], \n",
    "             5 : [28] , \n",
    "             10 : [132, 182], \n",
    "             23 : [0, 12, 28], \n",
    "             27 : [2] \n",
    "           } }\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "import os \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    " \n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_file_path = os.getcwd() + \"/ECTText/\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cust_stop_words = [\"'s\"]\n",
    "\n",
    "file_list = os.listdir(txt_file_path)\n",
    "\n",
    "# Adding custom words to the \n",
    "for temp in cust_stop_words:\n",
    "    stop_words.add(temp)\n",
    "\n",
    "inv_pos_index = {}\n",
    "\n",
    "# for i in range(1,3):\n",
    "for i in range(1,len(file_list)+1):\n",
    "    txt_file_name = txt_file_path + \"transcript-\" + str(i) + \".txt\"\n",
    "    filtered_tokens = [] \n",
    "    with open(txt_file_name,\"r\") as text_file:\n",
    "        text = text_file.read()\n",
    "        \n",
    "        # Tokenizing\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "            \n",
    "        # Stop-Word Removal + Lemmatization\n",
    "        for j in range(len(tokens)):\n",
    "            temp = tokens[j]\n",
    "            if(temp not in stop_words):\n",
    "                temp1 = lemmatizer.lemmatize(temp)\n",
    "                filtered_tokens.append((temp1,j)) # Storing their position in the original document as well for easier search\n",
    "\n",
    "        #  Building Corpus\n",
    "        for token in filtered_tokens:\n",
    "            if(token[0] not in inv_pos_index):\n",
    "                token_dict = {}\n",
    "                pos_list = [token[1]]\n",
    "                token_dict[i] = pos_list\n",
    "                inv_pos_index[token[0]] = token_dict\n",
    "            \n",
    "            else:\n",
    "                token_dict = inv_pos_index[token[0]]\n",
    "                if(i not in token_dict.keys()):\n",
    "                    pos_list = [token[1]]\n",
    "                    token_dict[i] = pos_list\n",
    "                    inv_pos_index[token[0]] = token_dict\n",
    "                else:\n",
    "                    token_dict[i].append(token[1])\n",
    "        \n",
    "        if(i%50==0):\n",
    "            print(str(i) + \" | \" + str(len(inv_pos_index.keys())))\n",
    "\n",
    "print(str(len(file_list)) + \" | \" + str(len(inv_pos_index.keys())))\n",
    "\n",
    "with open(\"Inv_Pos_index.json\", 'w') as fp:\n",
    "    json.dump(inv_pos_index, fp, sort_keys=True, indent=3)"
   ]
  },
  {
   "source": [
    "### Task 3.3\n",
    "- Build & Store a Permuterm Index (In Sorted Order)\n",
    "\n",
    "**NOTE**- Use imports from Task- (3.1 + 3.2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePermuterm(token):\n",
    "    perm_list = []\n",
    "    token = token + \"$\"\n",
    "    for i in range(len(token)):\n",
    "         perm_list.append(token)\n",
    "         token = token[1:] + token[0]\n",
    "    \n",
    "    return perm_list\n",
    "\n",
    "def GeneratePermutermList():\n",
    "    perm_list = []\n",
    "    \n",
    "    with open(\"Inv_Pos_Index.json\", 'r') as fp:\n",
    "        inv_pos_index = json.load(fp)\n",
    "        for token in inv_pos_index:\n",
    "            perm_list.extend(GeneratePermuterm(token))\n",
    "        \n",
    "        perm_list.sort()\n",
    "        return perm_list\n",
    "\n",
    "perm_list = GeneratePermutermList()\n",
    "\n",
    "with open(\"Permuterm.txt\",\"wt\") as text_file:\n",
    "    text_file.write(\" \".join(perm_list))"
   ]
  },
  {
   "source": [
    "## Task 4\n",
    "\n",
    "Wildcard Query Retieval\n",
    "\n",
    "### Output txt Format\n",
    "Let the query be trai*\n",
    "\n",
    "Let the two possible answers are train and trail.\n",
    "\n",
    "Let train appreared in doc 25 in 8 and 19 position and so on.\n",
    "\n",
    "We want the different terms to be separated by a colon \";\".\n",
    "\n",
    "So print your sample answer as:\n",
    "\n",
    "**`\"train:<25,8>,<25,19>,<29,1>,<29,6>; trail:<3,1>,<6,1>,<6,10> \\n\"`**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports Required\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Loading Stored Permuterms\n",
    "with open(\"Permuterm.txt\",\"r\") as permuterm_file:\n",
    "    text = permuterm_file.read()\n",
    "    perm_list = text.split()\n",
    "\n",
    "# Loading Posting List\n",
    "with open(\"Inv_Pos_Index.json\", 'r') as fp:\n",
    "    inv_pos_index = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRange(token, pos):\n",
    "    low_bound = pos\n",
    "    high_bound = pos\n",
    "    \n",
    "    while((perm_list[low_bound].startswith(token)) and (low_bound > 0)):\n",
    "      low_bound = low_bound - 1\n",
    "    \n",
    "    while((perm_list[high_bound].startswith(token)) and (high_bound < len(perm_list))):\n",
    "      high_bound = high_bound + 1\n",
    "    \n",
    "    return(low_bound+1, high_bound-1)\n",
    "\n",
    "def TokenSearch(token, start, end):\n",
    "    if(start==end):\n",
    "        if(perm_list[start].startswith(token)):\n",
    "            return (start, start)\n",
    "        else:\n",
    "            return -1\n",
    "    if(start>end or (end - start == 1)):\n",
    "        return -1\n",
    "\n",
    "    mid = int((start + end)/2)\n",
    "    if(perm_list[mid].startswith(token)):\n",
    "        token_range = FindRange(token,mid)\n",
    "    else:\n",
    "        if(token < perm_list[mid]):\n",
    "            token_range = TokenSearch(token, start, mid-1)\n",
    "        else:\n",
    "            token_range = TokenSearch(token, mid, end)\n",
    "    return token_range\n",
    "\n",
    "def QueryToToken(query):\n",
    "    # mon* CASE\n",
    "    if(query[-1] == \"*\"):\n",
    "        return query[:-1]\n",
    "    # *mon CASE\n",
    "    elif(query[0] == \"*\"):\n",
    "        return (query[1:] + \"$\")\n",
    "    # mon*mon CASE\n",
    "    else:\n",
    "        star_index = query.find('*')\n",
    "        return(query[star_index+1:] + \"$\" + query[:star_index])\n",
    "\n",
    "def TokenToWord(token):\n",
    "    if(token[-1]=='$'):\n",
    "        return(token[:-1])\n",
    "    else:\n",
    "        dollar_index = token.find(\"$\")\n",
    "        return(token[dollar_index+1:] + token[:dollar_index])\n",
    "\n",
    "with open(\"query.txt\",\"r\") as query_file:\n",
    "# with open(sys.argv[1],\"r\") as query_file:  # For taking the commandline input for the query text file\n",
    "    query_list = query_file.readlines()\n",
    "\n",
    "text_list = []    \n",
    "with open(\"RESULTS1_18CH3FP07.txt\",\"wt\") as text_file:\n",
    "        for query in query_list:\n",
    "            # Generating Token for Query\n",
    "            token = QueryToToken(query.rstrip(\"\\n\"))\n",
    "\n",
    "            # Searching for approriate range for the query given\n",
    "            token_range = TokenSearch(token, 0, len(perm_list)-1)\n",
    "\n",
    "            if(token_range == -1):\n",
    "                text_file.write(\"Query Not Found\")\n",
    "                text_file.write(\"\\n\")\n",
    "\n",
    "            else:\n",
    "                token_list = []\n",
    "                for i in range(token_range[0],(token_range[1]+1)):\n",
    "                    token_list.append(TokenToWord(perm_list[i]))\n",
    "\n",
    "                text = \"\"\n",
    "                for token in token_list:\n",
    "                    text+= token +\":\"\n",
    "                    token_dict = inv_pos_index[token]\n",
    "                    for doc in token_dict:\n",
    "                        posting_list = token_dict[doc]\n",
    "                        for posting in posting_list:\n",
    "                            text+= \"<\" + str(doc) + \",\" + str(posting) + \">,\"\n",
    "                    text = text[:-1]\n",
    "                    text+= \";\"\n",
    "                text_file.write(text[:-1])\n",
    "                text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}