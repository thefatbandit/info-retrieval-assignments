<div class="sa-art article-width " id="a-body"><p class="p p1">Ford Motor Company <span class="ticker-hover-wrapper">(NYSE:<a href="https://seekingalpha.com/symbol/F" title="Ford Motor Company">F</a>)</span>  Evercore ISI Virtual New Mobility &amp; AI Forum Conference September 22, 2020  9:00 AM ET</p>
<p class="p p1"><strong>Company Participants </strong></p>
<p class="p p1">Ken Washington - Chief Technology Officer</p>
<p class="p p1"><strong>Conference Call Participants </strong></p>
<p class="p p1">Chris McNally - Evercore ISI</p>
<p class="p p1">John Saager - Evercore ISI</p>
<p class="p p1"><strong>John Saager</strong></p>
<p class="p p1">Welcome back everyone. My name is John Saager from Evercore ISI. I cover autos, alongside my bold amigo and mobility partner in crime, Chris McNally. We're very happy to welcome Ford and in particular their CTO, Dr. Ken Washington. In his role as CTO, Dr. Washington is charged with overseeing the development and implementation of the company's technology strategy, as well as overseeing the Ford's next-gen vehicle architectures and controls and automated systems.</p>
<p class="p p1">For a quick background on Ford autonomous ambition, Argo AI was born out of Ford's 2017 investment in Argo, and has been developed in-house by Ford over the past several years until Q2 of this year, when Ford finalized an agreement with Volkswagen to create a joint venture. Argo now employs more than 1,000 people and its valuation sits at $7.5 billion. In terms of progress and timeline for their AV ambitions, Argo AI has a timeline that they've laid out and for ADAS Ford's product is called the Co-Pilot 360, which is their new assistant launching arm, the very exciting Mustang Mach-E in late '21 for a Level 2++ hands and eye go off system, also known as Active Drive Assist.</p>
<p class="p p1">With that, I welcome Dr. Ken Washington.</p>
<p class="p p1"><strong>Ken Washington</strong></p>
<p class="p p1">Thank you. It's great to be here. I look forward to the discussion and to addressing any questions that everyone has. </p>
<p id="question-answer-session" class="p p1"><strong>Question-and-Answer Session</strong></p>
<p class="p p1"><strong>Q - John Saager</strong></p>
<p class="p p1">Sure. I'll start with opening the discussion by talking about the pace of AV or automation within production cars and putting sort of Argo AI aside for now. If we focus on this path from Level 2 to Level 4, which the industry has largely pushed back over the years, particularly on Level 3 and Level 4, it raises two key questions for us. Knowing that there's little publicly available data, that's an accurate comparison of progress in the space, how can we think about milestones along the way? And then our second key question is, what are the key bottlenecks as you see it as we move from Level 2 to Level 3 and beyond? Is it sensor tech or AI and cost?</p>
<p class="p p1"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p1">Well, it's a super important topic. One of the things we've learned as we've continued our journey in AV is, when we began that journey, Level 4 and Level 2 were thought of as totally separate activities. And we're seeing those blend together more closely as sensors get more capable and as AI becomes more real. And one of the challenges that we're currently working on, that will be a key milestone is, is the challenge of resolving how can you regain the attention of a driver in a Level 3 solution. We are hard at work on that. We've got lots of activity working with technology partners, as well as our own team in Palo Alto and our team at Dearborn working together on an advanced perception stack as well as a solution that's in the cabin to determine how to regain the attention of the driver, if the attention is required, because some condition exists that the Level 3 system cannot handle. </p>
<p class="p p1">And just to remind everyone, a Level 3 solution is not a full SDS like a Level 4 solution that Argo is working on, it does require a driver to still be responsible in the event that they need to retake control of the vehicle. So I would say the key milestones along the way would be demonstrating that you can regain the attention of a driver if you're bringing a Level 3 solution to market. If you're in the Level 2 domain, we’ve solved most of the immediate challenges and that's why we’ve introduced to the market in our Mustang Mach-E, the Co-Pilot 360 and the Active Drive Assist, because we're really excited about the ability to bring the capability to do an advanced hands-off, eyes-off solution. </p>
<div class="p_count"></div>
<p class="p p2">So it's an exciting time, because the capabilities are getting much more mature, we're bringing those to market.</p>
<p class="p p2"><strong><span class="question">John Saager</span></strong></p>
<p class="p p2">Yes, for sure. So one of the things that we've talked about and as Ford is using some of these L2 and L3 or L2++ features in order to entice customers to lower their [biological] footprint, so it's great from our perspective to see that, all those features and Ford has sort of full technological capabilities on display in the Mach-E.</p>
<p class="p p2"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p2">Well, I was just going to say that the Mach-E is really -- going to have a really great Level 2 advanced driver assistance solution. But what we're really excited about is the next revolution, or the next evolution of the tech stack, which is going to allow us to do even more things with advanced driver assist, because we'll have shared memory of the modules of the vehicle. And we'll have more compute power and the ability to do more AI in the vehicle as we put together a software and an AI platform. So the future is even more exciting, so look forward to talking about that. </p>
<p class="p p2"><strong><span class="question">John Saager</span></strong></p>
<p class="p p2">Can you give us some more details around that just from your perspective, where does the software stack sit today? And where -- specifically in the next couple of years, what will change within that software stack and where will it end up?</p>
<p class="p p2"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p2">Sure. So the biggest change that's going to happen is, we're working to put together a centralized compute platform in the vehicle. So what I mean by that is, we're taking -- we're doing a decomposition of the sensor set, so that the intelligence is moved from the sensors and the actuators at the edge points in the vehicle to a centralized high performance compute cluster. We're also putting a fairly large shared memory module in this next-gen architecture, so that the sensors can share memory with each other. The reason that's important is the -- then you can provide updates over-the-air that will affect the performance of the sensors and actuators in a positive way. You can do intelligent user experience solutions. You can create those, both at job one and after job one. So over-the-air is going to take on much more importance in the future, because we'll have this centralized compute capability and shared memory capability. We'll also be moving towards zonal power architecture, so you can power up specific phones in the vehicle as needed. So imagine a solution that is aimed at a customer experience that needs to power up a particular zone, activate a camera, do some sensing, so it acts as a sentinel for you, or it does some kind of a service for you, and then power it down when it's done. That's not possible today, but it will be possible in the future tech stack in the vehicles that we will build on that. And what we're really excited about is, we're going to bring the tech stack that I'm talking about first to an all-new electric commercial vehicle. So it's a really exciting time. And that platform will look very different than the platforms of today. So we're only going to get better. </p>
<p class="p p2"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p2">That's great. And Ken, this is Chris. I mean, if we talk of -- if we continue on that idea of this path, right, you just laid out a really impressive sort of next-gen. Could we talk about some of the things we'll see as a glimpse of what's to come in your future of your active drive assist, maybe some of the new features? How that compares to what's on the market today, primarily with maybe some of your U.S. peers? And anything about how driver monitoring may be incorporated in sort of that transition product, which will still basically be a Level 2+ highway product?</p>
<p class="p p2"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p2">Yes. So we're doing some interesting things in the driver assist front. What we're working on now is, you referred to Level 2++, we're working to crack the barrier between Level 2++ and a true Level 3 solution for highway driving. So we're aiming to have that in the future. We haven't made an announce announcement date yet of course. But it's in our roadmap. So we're moving toward a solution that will allow you to truly be hands-off, eyes-off and mind-off so that you can do other experiences in the vehicle, you can turn your attention to read a book or whatever it might be. So the interior sensing capabilities will include the ability to regain your attention. And we're working on different ways to do that. This is a difficult problem. But we've got a team of more than 200 engineers in our Palo Alto lab, working with our team in Dearborn. And we even have a team in Aachen, Germany and in Israel, all working together on solving this problem. </p>
<div class="p_count"></div>
<p class="p p3">So that's a glimpse into what we'll be able to do in the future. So we're building on a pretty capable solution that we already have on the Mustang Mach-E that we're going to also bring to the new F-150. So we’re starting from a pretty good position. </p>
<p class="p p3"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p3">Ken, that's actually -- that’s pretty exciting because that's -- that really gets that transition phase, the driver attention, that’s really across the Level 3 conundrum. So we’re definitely going to come back to you on that over the coming months to see how we play on doing it. That seems really exciting. Maybe if we could talk a little bit about the sensor configuration in EyeQ drive assist, just as a number, physical cameras, radar. And then obviously sort of thinking a little bit ahead, in the past, I think you've had favorable comments about the nature of LiDAR technology. I think that was primarily around AV, the idea of it, with cost reduction that could come into active safety. So, we haven't seen it yet in a production car from Ford. So, sensor, radar, LiDAR, if you could just sort of give us an update what to expect in the drive-assist vehicle?</p>
<p class="p p3"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p3">Sure. So let me start with LiDAR, because that's the most straightforward. It's really important in our tech stack for retail and commercial vehicles for it to be affordable. And LiDAR at this point is still not yet at the cost point that we believe can be integrated into a tech stack for commercial or retail customer. It's really dedicated to -- right now, we're really leaving LiDAR to the platform for a SDS autonomous vehicle fleet that will be in a service where the return on costs will be viable business equation. In commercial and retail vehicles that we sell at scale, we believe we can provide a Level 2+ and a Level 3 solution using a clever mix of long range and short range radar, as well as high def cameras positioned both forward-facing as well as rear-facing. And then do a sensor fusion on the compute module that I spoke about. So once you have the ability to move the imaging of the radars and the cameras into a central compute module, you can do a pretty sophisticated sensor fusion and AI to do the perception required, to do a Level 2++ and a Level 3 solution. And that's exactly what we do in our Active Drive Assist.</p>
<p class="p p3">So, we have in our current -- on our current platform that we’re taking that approach and we expect the -- even the Level 3 solution to take a similar approach, where the added capability comes from enhanced AI processing not moving to LiDAR for a Level 3. So, we only move to LiDAR when we go to Level 4 and that’s what Argo is using for the tech stack in those vehicles.</p>
<p class="p p3"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p3">Okay. That's quite clear. So it's more about the AI, the compute, the fusion as opposed to perception being a bottleneck. Maybe...</p>
<p class="p p3"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p3">Correct.</p>
<p class="p p3"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p3">That's a different case. Okay. And then, two big themes and I'll pass it back to John. But what we've heard a lot over the last day is, obviously there’s big opportunity with Level 2+, you just outlined that. But also the main idea of reoccurring revenue streams, that opportunities, because this is now OTA, could you talk about how Ford is thinking about pricing optionality and functionality when it improves through over-the-air updates? Anything we can think about that, that sort of changes the way that cars are priced and their services are sold in the future?</p>
<p class="p p3"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p3">Yes. I really appreciate that question, because this is really, really a key part of our vision and our strategy. So in addition to today, where when you buy the vehicle on day one, you've got this amazing set of capabilities around driver assist and infotainment, that -- so you have right out of the gate, which includes things like Sirius radio and Wi-Fi hotspots. And on the DAT side, you've got Active Park Assist, interceptor assist, pre-collision detection, lane keeping with blind spot detection, Adaptive Cruise Control and Active Drive Assist as we've been talking about. So that is DAT, both of those, plus services that you can get after the fact through OTA. So we're really imagining -- we're envisioning the future where we can have additional services that are enabled by software that's bundled and then delivered like in the dongle over-the-air into the tech stack, that then we could provide a revenue stream for the company. But more importantly, we're providing a great go experience for the customer.</p>
<div class="p_count"></div>
<p class="p p4">So we're especially focused on creating a suite of services that we can deliver to our commercial vehicle customers. Ford is the number one commercial vehicle provider in the U.S. and Canada and Europe. And we only see that as being continued as a result of providing great services like additional work capabilities, imagine helping a small or medium businesses with helping them manage their business, managing the fleet of commercial vehicles that would exist for medium -- a small or medium customer.</p>
<p class="p p4">Look, we already have 5 million connected vehicles today. And with those 5 million vehicles, imagine they're all -- now they're all connected, right? And so we're starting to get data to understand how these customers use their vehicles. And so we can take that and translate it into new applications that'll improve how they use their vehicles, because we understand how they use them. Look, the bottom-line here is, with these over-the-air updates and the services we can provide, it'll help us make every insight count, every minute count that the driver and the customer needs to deliver high uptime for their customers on the commercial front. </p>
<p class="p p4">And then lastly, we really want to help our customers have make every interaction count that they have by building integrations into their business and software. So it's going to be an exciting future with the ability to deliver these kind of experiences over-the-air after job one. Think of it as an iceberg, where the tech stack and the technologies I've been talking about are the -- is the platform that sits under the water. And then above the water are the services that we can layer on top of that platform to do these things like customer business integrations and like leveraging cameras to do sentinel services and high uptime services, et cetera, et cetera. So, that's how we're thinking about the future.</p>
<p class="p p4"><strong><span class="question">John Saager</span></strong></p>
<p class="p p4">Thanks, Ken. That’s great. And just a reminder for everyone, you can shoot a question over there in the question box and we'll try to get it answered after more from Chris and I. Ken, I wanted to talk to you a little bit about electrical architecture, it’s been sort of an emerging theme at the conference over the past couple of days. Speaking with Aptiv and with Conti, Aptiv obviously has their SVA, the Smart Vehicle Architecture designed to sort of consolidate the multi-domain controllers in the vehicle. So I was wondering if you could talk a little bit about Ford's approach to that and if you're familiar with Aptiv or Conti program, just give some thoughts on their designs and their thoughts about how the future of electrical architecture of the vehicle will work?</p>
<p class="p p4"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p4">Well, first, let me say that it's pretty -- there are quite a few companies that are all looking to rethink the electric architecture of the vehicle platform, because of the times we live in, and Ford is no exception. I mean we're really leaning forward to design a next-gen electrical architecture that will allow us to deliver more features and experiences to our customer. And at the heart of that design is to centralize the compute from the edge points, as I mentioned earlier, to a centralized high performance compute cluster. And the high performance compute cluster will allow us to do things like compute the processed images and sensing data from multiple sources, so that we can deliver new experiences. </p>
<p class="p p4">Secondly, we'll have a shared memory module, zonal power in an AI software platform. And the first set of vehicles that already have the benefit of some of this thinking are -- is the F-150 and the Mach-E and the Bronco two and four door. So when you just take those four vehicles, we're going to start seeing lots of information and data at high scale because of the scale. And that's going to feed back to help us improve our designs and improve how we need to deliver new services to our customers. But within the electrical architecture itself, we're really focused on creating that platform that sits under the water and that iceberg model I talked about, so that we can deliver these kind of new services to our customers, and over the -- delivered over-the-air and to create new experiences.</p>
<div class="p_count"></div>
<p class="p p5"><strong><span class="question">John Saager</span></strong></p>
<p class="p p5">Right, makes sense. Then when we think about the emerging trends, EV and ADAS and as we move towards higher levels of autonomy, can you talk to us a little bit about some of the synergies that are available when you combine those ADAS levels with an EV like the Mach-E that maybe aren't available or improved in an EV versus internal combustion engine?</p>
<p class="p p5"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p5">Well, it's clear that, that an electric vehicle has a real simplicity about it that, that is -- that doesn't apply to an ICE vehicle. But what's really neat about the scale of Ford is that we're able to see and leverage the places where the two are, are similar, and turns out there’s quite a few parts of the electrical architecture that are common between an ICE and a BEV. So the IVI stack, for example, is not any different. The fact that you can merge and share memory from multiple sensors is not any different. The zonal power strategy that we'll take is -- only has one relatively minor difference having to do with the power needed to be distributed to the propulsion and the sustainability modules. So the AI software platform would be the same. So we want to be able to innovate on a software platform that can take the data and then put it and use it in a machine learning module to do something interesting and clever. </p>
<p class="p p5">So there's actually a lot of sharing that can happen across ICE platforms and BEV platforms. And we're going to leverage that to create the advantages of scale as we approach the development of our next-gen electrical architecture. So, it's a bit -- it's an interesting paradox. So while there's a lot of commonality, there's still advantage for us to really focus on the commercial BEV that we have in our plans to be the first platform to experience this next-gen electrical architecture in this new tech stack. So, we kind of have both of those things going for us, the power of scale, but also the ability to focus on the simplicity of a BEV commercial vehicle for our first instance. </p>
<p class="p p5"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p5">This is Chris. When we think about that next-gen commercial BEV and electrical architecture, could you frame how far we're going? Because obviously there's a wide spectrum, right, we could have a premium vehicle at 150 ECUs and I think everyone knows about how far you can go down with [types] under 10. It seems like the auto industry doesn't want to go that far that quick. But we're clearly moving towards more zonal controls. Could you have like an idea about how far down the line of ECU consolidation and zonal control you'll do in the next couple of years, five years? Because obviously, maybe over the next 10, that will go to more sort of vehicle level [OSSs]. </p>
<p class="p p5"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p5">Yes. And we’re kind of -- we're really looking today at the next say four to five years. So we're targeting 2025, ‘26 for the next-gen tech stack to come to market on this new commercial BEV platform. In that timeframe, we think we can go pretty far in terms of consolidation. We haven't finalized the whole -- the entire design, but we're pretty confident that we can get down to a small number -- single-digit number of central modules for doing the compute, and likely to be in the two to four range. </p>
<p class="p p5">So we think we can go pretty far. However, the compute-chip industry continues to innovate. Moore's Law is -- you know the famous saying, Moore's Law is dead, long live Moore's Law, continue to see innovation in the compute-chip arena, where new SOCs and new logic is being innovated every day. And so over the next 10 years, you could imagine that we'll be able to even go further in terms of module consolidation. But we've got our focus really on the next four to five years at the moment. And we think we can get down to two to four modules with respect to the compute stack, and then shared memory across all of the modules. So that's what we see in the next, say, four or five years. </p>
<div class="p_count"></div>
<p class="p p6"><strong><span class="question">John Saager</span></strong></p>
<p class="p p6">Okay, this is John, again. Last question from us before we kick it over to some of the audience Q&amp;A. We spoke earlier about partnerships, and talked to you about Ford's closest partnership, which is Argo AI, and Ford's AV LLC. Understand that those are not under your purview today. But what we really want to focus on is knowledge sharing. So how is Ford thinking about sharing the learnings between those groups? And if you could focus the discussion around data, this is an area where Ford should have some good overlap between the two programs.</p>
<p class="p p6"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p6">Yes, that's a really important question because the retail and commercial vehicles that we sell will have much more scale and volume than the Argo SDS vehicles. And so, one of the things where we're working on is, we're trying to work out how would we take the insights from connected car data that will be in the field from retail and commercial customers, and then leverage the inside of that to help Argo SDS Level 4 vehicles be more effective. We don't have all that worked out yet, but it is something we're thinking about very deeply. </p>
<p class="p p6">On the Argo front, their -- they -- and AV, a Level 4 AV, what has really differentiated between a Level 4 AV and a retail or a commercial vehicle is that, the amount of data that Level 4 AV collects and gathers is massive, and Argo has built a closed loop data ecosystem where they bring key insights from the LiDAR point cloud that each Level 4 AV that's being operated in on the streets brings back to the vehicle, gets processed and then gets brought back to the Argo engineering team, so they can continue to improve the Level 4 AV SDS.</p>
<p class="p p6">So, they have a pretty carefully architected technology stack for taking advantage of the massive amounts of data that an AV collects because of the LiDAR point cloud and the array of high-def cameras, as well as the radars that they use. So, I won't go into the details of how their stack works. It's not really my place to do that. But I am aware because I sit as observer on their Board that they have a very, very effective closed loop data ecosystem that has -- that enables their SDS to benefit from continuous learning and for perception as well as for navigation. So, learning between the two is something we're still working out, because we think we get the advantage both of us by having that kind of a solution. So more to come on that.</p>
<p class="p p6"><strong><span class="question">John Saager</span></strong></p>
<p class="p p6">Perfect. And I’ll -- we've got a bunch of questions here, so I'll try to combine a couple. Do maps have a role in sensor fusion in Level 2 to level 4, and if there a current timeline in place for moving to Level 4 automation from Argo testing to self-driving delivery vehicles?</p>
<p class="p p6"><strong><span class="answer">Ken Washington</span></strong></p>
<p class="p p6">So the Argo-Ford plan is to bring the Level 4 solution to a -- both a people-moving both a people-moving and a goods-moving service in the 2023 timeframe. Details of that we haven’t communicated but we are going to be operating in multiple cities, including Miami, Austin, Washington, D.C., Pittsburgh, Detroit and Palo Alto. So that -- those are the cities we're testing in today, that Argo is testing in. And we think -- well, we know this is the largest demographic footprint of any AV developer, and it's going to allow us to go to market in 2023 with a self-driving service for both people and goods in the ‘23 timeframe with confidence.</p>
<p class="p p6"><strong><span class="question">John Saager</span></strong></p>
<p class="p p6">And how do outside influences like smart cities, or 5G, sharing information between vehicles, V2X, communicating across the ecosystem play into your strategy?</p>
<p class="p p6"><strong><span class="answer">Ken Washington </span></strong></p>
<p class="p p6">So right now, that's not part of the Argo strategy, but it is part of the broader Ford strategy. So we really believe that 5G and cellular V2X will be key technology enablers for future smart vehicles, including advanced drive assist technologies. It will allow us to do Level 3 driving with more confidence in more places, including in urban environments. This is why we've made the central -- our Michigan Central Station investment and the Corktown investment. Our vision there is to have a mobility corridor that spans from Ann Arbor through Dearborn into Corktown which is in Downtown Detroit to be the development showcase for developing smart vehicles in a smart world where we can have 5G communications between vehicles and the infrastructure as well as cellular V2X communication amongst vehicles and the city streets.</p>
<div class="p_count"></div>
<p class="p p7">That's not in place today but we know it's coming. In fact, you can see signals of that in China where there are some early prototypes. We have done some early prototyping in China as well. So we know it’s coming. We're going to be an active player in that space. Look, we just showed an example of that in Detroit where we worked in partnership with Bedrock and Bosch, where we had a smart infrastructure in a garage with -- equipped with LiDAR sensors. And then we had intelligence in the vehicle, and the vehicle talked to the sensor, and the garage talked to the vehicle and allowed the vehicle to do a valet parking demonstration with the person holding a cellphone, telling the vehicle to go park itself. This is a glimpse of the future, where people will be able to do valet parking just by using their smartphone, but they'll also be able to do smart driving in an urban environment as well as on the highway as we're planning today. So 5G is going to be a huge and important part of that as well as cellular V2X.</p>
<p class="p p7">And by the way, since I'm answering this question, let me mention that cellular V2X is -- we have been really leaders in that space. We are the first auto company to plan to launch cellular V2X on our vehicles in 2022. And we've been active players in prototyping and confirming that the technology is superior to DSRC. So we're really excited about cellular V2X and that would be a stepping stone to 5G.</p>
<p class="p p7"><strong><span class="question">John Saager</span></strong></p>
<p class="p p7">Okay. Great. That was a great answer. Thanks for that. And then incorporating all of these systems within the instrument cluster, is that a key focus for Ford in terms of sustainability going forward?</p>
<p class="p p7"><strong><span class="answer">Ken Washington </span></strong></p>
<p class="p p7">Yes. So the instrument cluster and the infotainment stack is the first place where we’re going to see a lot of these customer experiences and innovations come to life. We are partnering with major technology companies. We have got a pretty exciting partnership that we haven't announced it yet, but stay tuned. We'll be communicating that before too long. But this partnership is going to allow us to bring industry-leading and differentiating experience to our IVI stack and our infotainment stack in the 2023 timeframe. This is going to be a really important stepping to the 2025, ‘26 next-gen tech stack I spoke about earlier. So this is going to get a lot of our attention in the next two years to bring that to our retail and commercial customers. So, it's going to be an exciting few years.</p>
<p class="p p7"><strong><span class="question">Chris McNally</span></strong></p>
<p class="p p7">Ken I think we have time for one more. So I’m going to take it for myself. One of the things we heard about yesterday which many think of you and maybe think of Ford, one of the LiDAR companies that came on, talked about the exciting opportunities in last-mile delivery. We've seen some of the press releases about Ford working on things like delivery bots, delivery drones. Can you talk a little bit about this whole idea of mobility from the software side to right to the door? How much -- how broad does Ford want to go when we think about these future opportunities?</p>
<p class="p p7"><strong><span class="answer">Ken Washington </span></strong></p>
<p class="p p7">That's a great last question, because it gives me the opportunity to talk about the fact that we -- at Ford, we are committed to working both in the now, the near and the far, in parallel. And we believe that in the future there will be end-to-end solutions like this that are mature, where you can make -- as I said earlier, commercial customer, make every minute count. So, robotic delivery is something we believe will be in the future. Today, it's in its infancy but that's why we have to work on technology today with partners and push to mature that technology so that it will be ready and we will be ready to receive an integrated solution for our customers. </p>
<p class="p p7">So I know that you've seen our partnership with Agility Robotics, where we've taken delivery of the first two next-gen Digit robots. This is a bipedal robot that actually we used in a demonstration to take a package out of a Ford van and walk to a doorstep and deliver the package. This is not production yet but this simple demonstration that we had at CES last year gives you a glimpse that the future is coming. And so, we're also working with a number of other robotic companies, as well as universities and other strategic partners. We will be -- the anchor tenants of a new research facility at the University of Michigan, opening this fall, called the Ford Motor Company robotics building on the University of Michigan campus. This is a really exciting partnership we have with the University of Michigan. We will use that facility and our partnership with the University of Michigan to help us accelerate the development of next-gen technologies like advanced robotics, drone technologies, bipedal robots, rolling robots. So all of this is coming and we're right in the middle of that mix, working in the far in parallel to working in the now and the near to deliver products today, as well as develop the possibilities for tomorrow. So thanks for that question. </p>
<div class="p_count"></div>
<p class="p p8"><strong>John Saager</strong></p>
<p class="p p8">Alright. With that, I think we'll wrap it up here. Thank you very much to Ken and the whole team at Ford. I think it was a great conversation and thanks everyone for attending. </p>
<p class="p p8"><strong>Ken Washington</strong></p>
<p class="p p8">Thank you.</p></div>