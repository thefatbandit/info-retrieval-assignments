<div class="sa-art article-width " id="a-body"><p class="p p1">Start Time: 12:15 January  1, 0000 12:46 PM ET</p>
<p class="p p1">NVIDIA Corporation <span class="ticker-hover-wrapper">(NASDAQ:<a href="https://seekingalpha.com/symbol/NVDA" title="NVIDIA Corporation">NVDA</a>)</span></p>
<p class="p p1">Evercore ISI Virtual New Mobility &amp; AI Forum</p>
<p class="p p1">September 21, 2020, 12:15 PM ET</p>
<p class="p p1"><strong>Company Participants</strong></p>
<p class="p p1">Paresh Kharya - Senior Director of Product Management in Data Center</p>
<p class="p p1">Simona Jankowski - VP, Head of IR and Strategic Finance</p>
<p class="p p1"><strong>Conference Call Participants</strong></p>
<p class="p p1">C.J. Muse - Evercore ISI</p>
<p class="p p1"><strong>C.J. Muse</strong></p>
<p class="p p1">Good morning and good afternoon, everyone. Thank you for joining us. My name is C.J. Muse at Evercore ISI and it is my distinct honor to host the NVIDIA team here today. We have Paresh Kharya, Senior Director of Product Management in Data Center; and also on the line is Simona Jankowski, VP, Head of Investor Relations and Strategic Finance.</p>
<p class="p p1">So, Paresh, thank you for joining us.</p>
<p class="p p1"><strong>Paresh Kharya</strong></p>
<p class="p p1">Thank you for having me, C.J. It’s my pleasure.</p>
<p class="p p1"><strong>C.J. Muse</strong></p>
<p class="p p1">Excellent. For the audience, there is a chat box where you can ask questions and I’ll do my best to integrate those questions into our fireside chat. But, Paresh, you’ve been busy; new product cycle, so I figured we will just start there with the new A100 offering. Ampere seems to be a transformative architecture and technology. Can you walk us through what this means for customers in terms of their ability to harness the power of AI?</p>
<p class="p p1"><strong>Paresh Kharya</strong></p>
<p class="p p1">Yes. So, C.J., if you look at, there are two megatrends in AI. The first trend is the exponential growing complexity of AI models. At the launch of A100 when we spoke, we talked about how since the launch of our Volta, the model complexity has grown 3,000x to train the largest models. In fact, that trend continues. Just a month after we announced Ampere, OpenAI introduced a model called GPT-3. It’s 175 billion parameter model and that’s 10x more compute intensity to train these models. So that continues – that presently continues. 30x more compute intensity to train the largest models since we launched Volta. And this requires large scale computers, because otherwise it would just take years to train these massive models. So that’s the first trend.</p>
<p class="p p1">The second trend is because these AI models are now so accurate, there is a pervasive AI powered applications everywhere from conversational AI to recommended systems and so on and these service millions of users. And as they service millions of users, each interaction with a user requires a tiny amount of acceleration. So it requires many small scale accelerated computers. Now these two are seemingly divergent needs, but A100 for the first time provided a single solution for both of these divergent needs at the same time.</p>
<p class="p p1">First, it provides a massive performance boost; 20x higher performance compared to Volta. Second, it provides 50x higher scalability. So a single server can be used as one big GPU working on larger scale training problems or can be interconnected over the network to do even larger scale training problems. And secondly, with the capability called Multi-Instance GPU, a server can be partitioned into 50 different smaller scale GPUs, each running a different inference application. So for the first time, A100 unifies data center acceleration for both training and inference and that’s why our customers are just loving the A100 architecture and we’ve really seen a rapid uptake.</p>
<p class="p p1"><strong>C.J. Muse</strong></p>
<p class="p p1">That brings me to the next question which was can you share with us the reception so far? And I guess I’d love to hear whether it’s scale up, scale out or other drivers as you think about the adoption for hyperscale versus enterprise versus supercompute, what excite you to each of those segments perhaps more than something else?</p>
<p class="p p1"><strong>Paresh Kharya</strong></p>
<p class="p p1">Sure. So there’s been a tremendous uptake. A100 came to the cloud faster than any NVIDIA GPU in the history. It’s already available on Google Cloud, on Microsoft Azure and it’s coming soon on every other hyperscale and other cloud providers. There have been over 50 different A100 server models that were announced by the leading server manufacturers within a few weeks of our launch, and each of these will help the vertical industries ramp up over the next coming quarters. And the value proposition is really strong. So for hyperscale with Multi-Instance GPU, A100 enables great economics, as we discussed, 50x higher scalability as well as a unified training and inference platform.</p>
<div class="p_count"></div>
<p class="p p2">So they can provision instances of different capability from the same infrastructure. And that's why cloud providers and hyperscalers are so excited to bring A100 to their clouds. For enterprises, this enables unification of acceleration in their data center so they can optimize with a single architecture, while optimizing the utility and utilization. Because the same investment to accelerate the data center can accelerate a full range of applications, whether it's data analytics, artificial intelligence, AI inferencing, virtual graphics, all of that can be accelerated by the same infrastructure and the same investment.</p>
<p class="p p2">Finally, for supercomputing you mentioned, we introduced next generation of NVLink and combined with our Mellanox InfiniBand technology, along with our software stack that we call Magnum IO, we can scale up applications massively, something that supercomputing really enjoys; multi-GPU, multi-node, really large scale applications, all backed up by the same single architecture that they've come to rely upon from NVIDIA. And we've had several design wins in the supercomputing because of this reason. In U.S., for example, with a supercomputer called Perlmutter from NERSC or in Europe with [Juelich] [ph] supercomputing, with Max Planck Institute and so on, we had several designs wins with the A100.</p>
<p class="p p2"><strong>C.J. Muse</strong></p>
<p class="p p2">Congratulations on that. I was hoping to pivot to your system level approach, which I think is something that is perhaps still underappreciated for NVIDIA, but I'm thinking less so as every day passes. So, obviously, acceleration of computing starts with the GPU, but it continues through system designs, system software algorithms and optimized applications. Can you speak to the importance of this platform approach for NVIDIA?</p>
<p class="p p2"><strong>Paresh Kharya</strong></p>
<p class="p p2">Yes, the platform approach is critical. We make great GPUs. Tensor Core, now in its third generation, is unrivaled in terms of performance or NVLink provides really massive speed up in a multi-GPU environment and so on. But it's the 2 million NVIDIA developers that are rapidly growing and that's a testament to our platform approach. So we provide our fully end to end platform to our developers, CUDA, which is a single programming model that sits on top of our GPUs and CUDA-X, which is our layer of domain specific libraries that provide the map that's needed for the different domains of applications, whether it's AI training, inferencing, running larger scale jobs, and so on. It's that layer of CUDA-X, which has millions of lines of code and it has evolved for over 15 years now. That's really important. And AI frameworks, they sit on top of it.</p>
<p class="p p2">So frameworks like TensorFlow and PyTorch, the reason why they are able to perform so well on our GPUs is because of the layer CUDA-X, which contains all the acceleration libraries that make these frameworks work well. But we are not stopping there. We also have now created application frameworks that are really important for enterprises to deploy and create applications, whether it's conversational AI applications with our framework called Jarvis, or recommendation applications with our framework called Merlin and so on, and all of this software stack that we have is delivered through NGC, which is our hub for GPU optimized software.</p>
<p class="p p2">So the net effect is, with the full platform approach, we are able to sustain a virtuous cycle of adoption. Because the more developers, because they find it easy to develop on our platform, it implies creating more applications and more deployed endpoints; because there are more deployed endpoints, it attracts even more developers on our platform. So because of our full platform approach, we are able to sustain this virtuous cycle of adoption that's so vital in this industry.</p>
<div class="p_count"></div>
<p class="p p3"><strong>C.J. Muse</strong></p>
<p class="p p3">Okay. I guess playing devil's advocate here talking to some folks in the industry, though clearly I think this is more one to two years ago, there's a notion that if you can just support one or two AI frameworks that that's enough and that CUDA-X isn't necessary. So, can you speak to why you disagree with that notion?</p>
<p class="p p3"><strong>Paresh Kharya</strong></p>
<p class="p p3">Yes. So the reason why frameworks like TensorFlow and PyTorch work so well on our platform is because of CUDA and CUDA-X. These frameworks sit on top of our CUDA and CUDA-X and they automatically benefit from all the capabilities that we build in our hardware. For example, we launched our third generation of Tensor Core GPUs with Ampere architecture and we introduced a new precision called TF32. There is zero code change that is required from the frameworks [indiscernible] precision that we introduced comes to developers.</p>
<p class="p p3">So the CUDA and CUDA-X are really important in accelerating all the frameworks. This is the reason why, frankly, our customers confidently invest in our platform. Because they've seen our commitment with our libraries and our single CUDA programming stack, they know that with every generation of our GPU, they'll automatically benefit from the architecture, sometimes without changing a single line of code. And that's really the benefit that you get when you have platform in layers with the industry framework sitting on top of things like CUDA and CUDA-X.</p>
<p class="p p3"><strong>C.J. Muse</strong></p>
<p class="p p3">It’s a great breakdown and helpful explanation. I guess curious as you think about, again, platform approach, are you seeing any differences in terms of what is required by hyperscalers versus your enterprise customers? And I know that we're early in the enterprise ramp for the A100, but would be curious if you're seeing any sort of differences there?</p>
<p class="p p3"><strong>Paresh Kharya</strong></p>
<p class="p p3">Yes. So with our platform approach, the great advantage we have is we are able to meet where our customers are. Meaning on one hand, we create our end to end solutions like DGX for enterprises but at the same time we open up our platform with HGX which is the guts behind the DGX that hyperscalers deploy as they create their infrastructures, as well as our OEM server maker partners who work with us at that layer. So that's on the hardware front. And if you look at the software stack, with the hyperscalers, our approach is to deeply integrate into their software stacks, into the frameworks that they are promoting. So, for example, let's take TensorRT. It's our library for optimizing train models for inference. It's deeply integrated with TensorFlow.</p>
<p class="p p3">Similarly, it's deeply integrated with Microsoft Onyx runtime. So with the hyperscalers, our approach is to deeply integrate our software stack. On the other side, hyperscalers also contribute to our software stack. Just last week, for example, Microsoft announced DeepSpeed. It's used for extreme scale training of models on thousands of GPUs, for models that reach trillions of parameters, really important contribution by Microsoft and it benefits, because it's an open source project, it benefits every NVIDIA customer. So that's our strategy with hyperscalers. For enterprises, on the other hand, they don't always have in-house AI or software capabilities. So our approach is to continue to provide higher level stacks and value prop. NGC is a critical part of that.</p>
<p class="p p3">With NGC, we provide pre-trained models, so enterprises don't have to start from scratch with no training models and so on. They can just take pre-trained models that are state of the art, whether it's for conversational AI or recommenders, and so on and they can apply their own proprietary data to customize it to their needs using what's called transfer learning. We also provide application frameworks which are again targeted for enterprises, which have end to end workflows, like how we were describing whether it's Isaac for robotics or Clara for healthcare. So while hyperscalers led the adoption of AI in accelerated computing, enterprises are now on that same adoption curve. And for many, it's an existential imperative, frankly. And we've seen tremendous growth in the vertical industry. Our data center business derives close to half of our revenues from enterprises.</p>
<div class="p_count"></div>
<p class="p p4"><strong>C.J. Muse</strong></p>
<p class="p p4">That’s helpful. So you made a splash announcement I guess a week back with the potential Arm acquisition. Curious as you think about full stack solution end to end, how does that asset fit into your go-to-market strategy and how excited are you to potentially have CPU for the data center in your portfolio?</p>
<p class="p p4"><strong>Paresh Kharya</strong></p>
<p class="p p4">Yes. So, as you pointed out, first, it creates a premier computing company for the age of AI by combining our AI computing platform with Arm’s vast CPU ecosystem. Secondly, it expands Arm’s IP licensing portfolio with NVIDIA technology in several large end markets, like mobile and PC. And thirdly, it turbo-charges Arm’s server CPU roadmap pace which can then further accelerate the data center in edge AI and IoT opportunities for us together.</p>
<p class="p p4">And finally, it expands the reach of our computing platform from 2 million developers that we have today to 50 million developers on Arm. And if you look at from an endpoint perspective, we sell about 100 million or so chips per year. ARM architecture based chips sell 22 billion per year. So ultimately, the benefit is the virtuous cycle; more endpoints and applications will attract more developers, and more developers will result in even more applications and more deployed endpoints. And so that's the exciting possibility here.</p>
<p class="p p4"><strong>C.J. Muse</strong></p>
<p class="p p4">That's great. I appreciate that. Hoping to pivot now to different business verticals. I think something that's special to NVIDIA is that you're clearly taking a customized approach to each vertical. And I guess can you speak to which businesses are currently the fastest to adopt accelerated computing for AI? And how are they benefiting from your platform?</p>
<p class="p p4"><strong>Paresh Kharya</strong></p>
<p class="p p4">Yes. So we're seeing adoption across retail, industrial, automotive, healthcare, video analytics. As we were discussing, close to half of our data center business is now driven by vertical industry and the use cases and the value prop has been tremendous. If you look at retail, for example, Walmart deploys NVIDIA AI for optimizing their supply chain. The problem is immense for customers like Walmart. They have 100,000 different products that are going into thousands of their stores just in U.S.</p>
<p class="p p4">And if you look at how much they have to stock to minimize the stock outs and optimize the shelf space utilization and so on, on a weekly basis, they are predicting demand for 0.5 billion item by store combinations. And each of that combination is impacted by a number of factors. So they have a massive sort of data analytics and AI training problem. Using NVIDIA platform, they're able to increase their data analytics by over 100x and that results in faster delivery to stores and so on.</p>
<p class="p p4">Similarly, if we look at on the manufacturing side, we talked about how we're working with BMW, where BMW produces 10,000 cars every day. And the challenge is the challenge of customization. They offer 40 different car models, each with 100 different options per car. So the logistics problem is just humongous. They have tens of millions of parts that are coming every day from hundreds of suppliers and they have to deploy that in their factories to produce 10,000 built to order cars every day.</p>
<p class="p p4">And in order to keep all these production lines humming and operating smoothly, the parts need to arrive just in time, just in sequence. And for that, they selected our Isaac robotics platform to enhance automation in their automotive factories. So really amazing possibilities with what the enterprises are now able to do and transform their business models and operations with AI.</p>
<p class="p p4"><strong>C.J. Muse</strong></p>
<p class="p p4">I guess sticking with the vertical and really digging a bit deeper into the data analytics and data science side, in the past you've spoken about coordination of CPU servers creating a bottleneck with the exponential data growth. Can you walk us through this dynamic and how does supporting Spark 3.0 solve this?</p>
<div class="p_count"></div>
<p class="p p5"><strong>Paresh Kharya</strong></p>
<p class="p p5">Yes. So before a model can be trained, the data needs to be prepared for it. And this is a process that's called as ETL in the industry which stands for extract, transform and load, meaning data is coming from various sources and you are preparing that data, readying it, so it can be used to train these models. But this is a very computationally intensive process. In fact, 70% to 80% of data scientists’ time today is spent just preparing for data, not even training, just preparing for data. So data analytics is a big computing challenge. And Spark is the most popular data analytics framework for distributed computing for distributed data analytics. There are 16,000 enterprises and 0.5 million data scientists that use data that uses Spark for data analytics, primarily running on CPU servers.</p>
<p class="p p5">Now, with the Spark 3.0, we are bringing GPU acceleration to this vital part of an end to end AI development pipeline. And the speedups are tremendous. Databricks, for example, is offering GPU accelerated Spark on their platform. Google is offering it on Cloud Dataproc. The way this benefits is to twofold. First, there's faster performance. So now you can do all of the data analytics much faster, you are able to as customers – our customers are able to iterate faster and they are able to train their models faster. We talked about the Walmart example earlier. They are predicting their – let's say they're predicting their supply chain on a weekly basis. But now if you're able to optimize your models and analyze them faster, what if you can bring down that prediction to a day. Now you can optimize supply chains even further.</p>
<p class="p p5">Secondly, because now you have the same infrastructure to both analyze the data as well as train, you can reduce infrastructure cost. Same infrastructure can do data analysis and training. Adobe, for example, talked about how using Spark 3.0, they were able to achieve 7x higher performance and at the same time 90% savings in the cost. So, Spark 3.0 in GPU acceleration is really transformative. It's a very important part of the overall AI and data analytics workflow that we’re now able to accelerate for our customers.</p>
<p class="p p5"><strong>C.J. Muse</strong></p>
<p class="p p5">And if you think about the economic benefit to NVIDIA, is it more selling GPUs for the ETL process or is it TAM expansion for AI workloads?</p>
<p class="p p5"><strong>Paresh Kharya</strong></p>
<p class="p p5">That's a really great question. The way to look at this C.J. is because the data analytics can be done much faster, you can do a lot more iterations. So instead of having training jobs run on a weekly basis, for example, you can do it on a daily basis, you can do it in some cases even in real time. So the impact is really expansionary because you are now able to do more of the training and automation. So it expands the overall opportunity. At the same time, because it helps customers optimize their supply chains, optimize their business operations, they are able to invest even more into the infrastructure in order to do the combined data analytics in training.</p>
<p class="p p5"><strong>C.J. Muse</strong></p>
<p class="p p5">And I guess just a last question on Spark 3.0. How does that fit with NVIDIA's overarching goal in making the data center the computing unit?</p>
<p class="p p5"><strong>Paresh Kharya</strong></p>
<p class="p p5">Yes. So data center is fast becoming the unit of computing. And what's meant by that is developers today are writing applications that are not limited to the confines of a single server. The modern applications that are being deployed today, these are micro services based and they run everywhere in the data center. In fact, in some cases, they are running on the hybrid cloud as well. So the on-premise, the cloud and the edge is no longer siloed. There are all hybrid applications that are taking advantage of each as needed. So Spark, in particular, is a great example of distributing data analytics processing. And similarly, Kubernetes is used to manage all of these clusters at the data center scale.</p>
<div class="p_count"></div>
<p class="p p6">And so the data center needs to be software defined. Because we are now able to accelerate Spark, we are able to accelerate a very important framework that is used for distributed processing in the data centers. And so it accelerates this transition towards the modern data centers of the future that are optimized for the full data center scale. We've also talked about how this modern data center requires three pillars; there’s CPU, there’s GPU and there’s DPU. A CPU for running sequential tasks and hosting the operating system; GPU for accelerating all these modern applications, whether it's data analytics or machine learning or AI; and DPU for processing the data in transit and securing all of that communication.</p>
<p class="p p6"><strong>C.J. Muse</strong></p>
<p class="p p6">Very helpful. On the enterprise side, and I guess maybe focusing a bit on the virtualization side, enterprise compute is still vast majority of the computing market. Can you talk about what you're doing to make enterprise computing look more like how hyperscalers run their cloud?</p>
<p class="p p6"><strong>Paresh Kharya</strong></p>
<p class="p p6">Yes. So enterprises, as you know, enterprises run 75% of their applications on-premise. However, they are being impacted in very profound ways by cloud computing. And so they are now finding themselves at an inflection point. And there are three sort of major changes, if you will, that are driving this modern enterprise infrastructure modernization; first, AI and data-driven applications. Enterprises are moving from few experimental AI applications to a future where every enterprise application will be AI infused and constantly improved with the data-driven insights.</p>
<p class="p p6">Secondly, all the monolithic applications are giving way to micro-services-based modern applications. And finally, we talked about the notion that on-premise, cloud and edge are no longer siloed. There are hybrid applications that are seamlessly taking advantage of each as needed for the application. So the way we are addressing that is on the server and the hardware side, we have DGX SuperPOD, which is basically data center as a product. So enterprises can stand up a world class AI first data center in a matter of weeks. Secondly, for mainstream and edge computing, we have EGX which is helping enterprises do the processing closer to the source of data.</p>
<p class="p p6">And on the software side, we are – and the platform side, we are working with the leading partners in the industry. So, on one hand, we’re working with Kubernetes ecosystem to make GPUs a first class citizen here, working with downstream partners like Red Hat OpenShift to make GPU acceleration natively supported, and also with our partners like VMware for mainstream enterprise computing. Last year, we announced virtual compute server that enables VMware vSphere to be used as the management layer for AI and data analytics workloads that are GPU accelerated. And finally, we make all of our software stacks available as containerized stacks for enterprise deployment to help with this transition to the modern infrastructures at enterprises.</p>
<p class="p p6"><strong>C.J. Muse</strong></p>
<p class="p p6">So we only have about five minutes left, so I figured I'd kind of conclude with maybe some larger picture questions. So where do you think we are in [indiscernible]?</p>
<p class="p p6"><strong>Paresh Kharya</strong></p>
<p class="p p6">Yes. So AI is still very nascent. And you can see that it's still nascent is because it continues to advance exponentially. Compute complexity is up 30,000x over just last four years, for example. And it's opening up new markets as these giant models can now be trained and some wonderful use cases are now possible. From our roadmap, we are driving our roadmap and that opens up the possibilities for developers. In terms of interesting applications, conversational AI, for example, it's highly accurate now. It's really changing the way we interact with applications and machines.</p>
<div class="p_count"></div>
<p class="p p7">One of the companies closer to the investment community, for example, is Kensho. It's a S&amp;P global company. They have a speech recognition product, or ASR product called Scribe that transcribes earnings calls, but it customizes these models for the financial jargon. So tens of thousands of earnings calls, management presentations, acquisition calls each year can be transcribed with really high accuracy, and it helps improve the coverage of all of these calls.</p>
<p class="p p7">Similarly, the other interesting use cases are with recommendation systems, for example. Alibaba, the largest e-commerce company, they have massive volumes, just mind boggling. On their Single’s Day event, for example, they did $38 billion in sales. That's 2x the Black Friday and Thanksgiving online shopping combined. And because they were able to deploy advanced recommendation systems powered by NVIDIA GPUs, they had 6x more complex models that they were able to deploy for recommendations that improved their click through rate by 10%. So a phenomenal value.</p>
<p class="p p7">On the healthcare side, for example, we are looking at – cold and flu season is upon us. We’re still in the pandemic. And so we’re working – we work with NIH to create an AI that can take the chest CT scans and help identify or classify the cause of pneumonia. Pneumonia can be caused by bacterial infections, fungal, viral. We want to be able to detect whether a pneumonia is caused by COVID-19 or by other causes.</p>
<p class="p p7">And with working with NIH, we are able to create this model that detects with over 90% accuracy when a pneumonia is due to COVID-19. And we made this model available on NGC for anyone to deploy. So use cases are immense and we are still in the nascent phases as we drive our roadmap. It opens up new possibilities for developers and what's possible. So we're very excited about this.</p>
<p class="p p7"><strong>C.J. Muse</strong></p>
<p class="p p7">Okay. So we’ve got about 50 seconds left and our last question for you and wherever you want to go, but what do you think is underappreciated about either AI or in video or both?</p>
<p class="p p7"><strong>Paresh Kharya</strong></p>
<p class="p p7">Sure. I would say there are three things. First, the pace of adoption and the scale of GPU usage in everyday applications is really underappreciated. One quick example is Microsoft recently talked about how BERT, which is just a model that came a year and a half ago now powers all the search in Bing, their search engine, and it's only made possible by GPUs, thousands of GPUs are being used to serve the search results. So from research to adoption, the pace is just mind boggling. Second, AI is not just used in the consumer Internet and hyperscale space.</p>
<p class="p p7">Enterprises are adopting AI very rapidly. We talked about several examples. Half of our revenues now come from the enterprise segment. And finally, the importance of full stack is underappreciated. It starts with a great chip, but it needs a full software platform to make all of that work and happen. And at NVIDIA, we continue to innovate on the architecture, while our software platform enables developers to access all of that hardware capability seamlessly as we go from one architecture to the other, and our NGC simplifies the development and deployment to all of our customer base.</p>
<p id="question-answer-session" class="p p7"><strong>Question-and-Answer Session</strong></p>
<p class="p p7"><strong><span class="question"> </span></strong></p>
<p class="p p7"><strong><span class="answer"> </span></strong></p>
<p class="p p7"><strong>C.J. Muse</strong></p>
<p class="p p7">Paresh, that was fantastic. Unfortunately, we’ve run out of time. But thank you. I really appreciate it and wish you the best and great health in these still crazy pandemic times.</p>
<p class="p p7"><strong>Paresh Kharya</strong></p>
<p class="p p7">Thank you, C.J.</p></div>