{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda1c5afe4909b94d1f9ad8012633fc5cb5",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pickle5 as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    " \n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_path = parent_dir + \"/Dataset\"\n",
    "\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Adding custom words to the stop-words list\n",
    "cust_stop_words = [\"'s\"]\n",
    "for temp in cust_stop_words:\n",
    "    stop_words.add(temp)\n",
    "\n",
    "with open(parent_dir + '/Leaders.pkl','rb') as fp:\n",
    "    global leader\n",
    "    leaders = pickle.load(fp)\n",
    "\n",
    "global inv_pos_index\n",
    "inv_pos_index = {}\n",
    "lead_inv_pos_index = {}\n",
    "word_vec = []\n",
    "for i in range(len(file_list)):\n",
    "    filtered_tokens = []\n",
    "    txt_file = open(data_path + \"/\" + str(i) + \".html\")\n",
    "    soup = BeautifulSoup(txt_file, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Tokenizing\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Stop-Word Removal + Lemmatization\n",
    "    for token in tokens:\n",
    "        if(token not in stop_words):\n",
    "            filtered_tokens.append(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    # Adding the doc_text for finding leaders\n",
    "    word_vec.append(filtered_tokens)\n",
    "    \n",
    "    # Finding term-freq of each token in the token-list\n",
    "    token_freq = Counter(filtered_tokens)\n",
    "\n",
    "    # Adding the (d,tf) pair to the Inverted Positional Index\n",
    "    for token in token_freq:\n",
    "        tf = math.log10(1+token_freq[token])\n",
    "        if(token not in inv_pos_index):\n",
    "            inv_pos_index[token] = [0,tuple((i,round(tf,5)))]\n",
    "            # Adding to the leader inv_pos_index\n",
    "            if(i in leaders):\n",
    "                lead_inv_pos_index[token] = [0,tuple((i,round(tf,5)))]\n",
    "        else:\n",
    "            inv_pos_index[token].append(tuple((i,round(tf,5))))\n",
    "            # Adding to the leader inv_pos_index            \n",
    "            if(i in leaders and token not in lead_inv_pos_index):\n",
    "                lead_inv_pos_index[token] = [0,tuple((i,round(tf,5)))]\n",
    "            elif(i in leaders and token in lead_inv_pos_index):\n",
    "                lead_inv_pos_index[token].append(tuple((i,round(tf,5))))\n",
    "\n",
    "\n",
    "\n",
    "# Computing & Storing the idf value for each term\n",
    "for token in inv_pos_index:\n",
    "    df = len(inv_pos_index[token])-1\n",
    "    idf = math.log10(float(len(file_list)/df))\n",
    "    inv_pos_index[token][0] = idf\n",
    "\n",
    "#  Setting idf values for leader inv_pos_index\n",
    "for token in lead_inv_pos_index:\n",
    "    lead_inv_pos_index[token][0] = inv_pos_index[token][0]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"Inv_Pos_Index.json\", 'w') as fp:\n",
    "    json.dump(inv_pos_index, fp, sort_keys=True, indent=3)"
   ]
  },
  {
   "source": [
    "## Building Champion Lists\n",
    "\n",
    "- **ChampionListLocal:** Top 50 tf value document list per-term.\n",
    "\n",
    "- **ChampionListGlobal:** Top 50 tf + g(d) value document list per-term. \n",
    "\n",
    "\n",
    "***NOTE:*** g(d) values are taken from StaticQualityScore.pkl"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary Code -------------------------------------------------------------------------------------------------\n",
    "file_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_path = parent_dir + \"/Dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"Inv_Pos_Index.json\", 'r') as fp:\n",
    "    inv_pos_index = json.load(fp)\n",
    "\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "with open(parent_dir + '/Leaders.pkl','rb') as fp:\n",
    "    global leader\n",
    "    leaders = pickle.load(fp)\n",
    "# Temporary Code --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Champion List Local\n",
    "global champ_list_local\n",
    "champ_list_local = {}\n",
    "for token in inv_pos_index:\n",
    "    temp = inv_pos_index[token][1:]\n",
    "    temp.sort(key = lambda x: x[1],reverse = True)\n",
    "    temp = [tuple(doc) for doc in temp[:min(50,len(temp))]]\n",
    "    champ_list_local[token] = temp\n",
    "\n",
    "\n",
    "# Champion List Global\n",
    "global champ_list_global\n",
    "champ_list_global = {}\n",
    "with open(parent_dir + '/StaticQualityScore.pkl','rb') as fp:\n",
    "    g = pickle.load(fp)\n",
    "\n",
    "    for token in inv_pos_index:\n",
    "        temp = inv_pos_index[token][1:]\n",
    "        temp.sort(key = lambda x: x[1] + g[x[0]],reverse = True)\n",
    "        temp = [tuple(doc)  for doc in temp[:min(50,len(temp))]]        \n",
    "        champ_list_global[token] = temp\n",
    "\n",
    "# Calculating sum of squares\n",
    "Vd_norm = [0]*len(file_list)\n",
    "\n",
    "for token in inv_pos_index:\n",
    "    temp = inv_pos_index[token]\n",
    "    for i in range(1,len(temp)):\n",
    "        doc = temp[i][0]\n",
    "        tidf = temp[i][1] * temp[0]\n",
    "        Vd_norm[doc] = Vd_norm[doc] + (tidf**2)\n",
    "    \n",
    "#  Calculating sqrt for Vd_norm\n",
    "for val in Vd_norm:\n",
    "    val = math.sqrt(val)"
   ]
  },
  {
   "source": [
    "## Query Processing & Scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"query.txt\",\"r\") as query_file:\n",
    "# with open(sys.argv[1],\"r\") as query_file:  # For taking the commandline input for the query text file\n",
    "    query_list = query_file.readlines()\n",
    "\n",
    "def tf_idf(filtered_query,pos_index):\n",
    "    score_dict = {}\n",
    "    Vq_norm = 0\n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "            Vq_norm = Vq_norm + inv_pos_index[query][0]\n",
    "    Vq_norm = math.sqrt(Vq_norm)    \n",
    "\n",
    "    # Calculating tf-idf scores\n",
    "    for query in filtered_query:\n",
    "        if(query in pos_index):\n",
    "            doc_list = pos_index[query][1:]\n",
    "            for doc in doc_list:\n",
    "                tf = doc[1]\n",
    "                idf  = pos_index[query][0]\n",
    "                tf_idf = (idf * (tf*idf))/(Vq_norm * Vd_norm[doc[0]])\n",
    "            \n",
    "                if(doc[0] in score_dict):\n",
    "                    score_dict[doc[0]] = score_dict[doc[0]] + tf_idf\n",
    "            \n",
    "                else:\n",
    "                    score_dict[doc[0]] = tf_idf\n",
    "\n",
    "    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return score_dict[:10]\n",
    "\n",
    "def champ(filtered_query, score_type):\n",
    "    if(score_type == 'local'):\n",
    "        champ_dict = champ_list_local\n",
    "    elif(score_type == 'global'):\n",
    "        champ_dict = champ_list_global\n",
    "    else:\n",
    "        print(\"champ_score() wrong call\")\n",
    "        return\n",
    "\n",
    "    score_dict = {}\n",
    "    Vq_norm = 0\n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        Vq_norm = Vq_norm + inv_pos_index[query][0]\n",
    "    Vq_norm = math.sqrt(Vq_norm)    \n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        doc_list = champ_dict[query]\n",
    "        for doc in doc_list:\n",
    "            tf = doc[1]\n",
    "            idf  = inv_pos_index[query][0]\n",
    "            tf_idf = (idf * (tf*idf))/(Vq_norm * Vd_norm[doc[0]])\n",
    "        \n",
    "            if(doc[0] in score_dict):\n",
    "                score_dict[doc[0]] = score_dict[doc[0]] + tf_idf\n",
    "        \n",
    "            else:\n",
    "                score_dict[doc[0]] = tf_idf\n",
    "    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return score_dict[:10]\n",
    "\n",
    "def list_to_string(score_list):\n",
    "    text = \"\"\n",
    "    for doc in score_list:\n",
    "        text = text + '<' + str(doc[0]) + ',' +str(round(doc[1],5)) + '>,'\n",
    "\n",
    "    text = text[:-1] + '\\n'\n",
    "    return text\n",
    "\n",
    "follow_list = find_followers(leaders, lead_inv_pos_index)\n",
    "\n",
    "with open(\"RESULTS1_18CH3FP07.txt\",\"wt\") as text_file:\n",
    "    for query in query_list:\n",
    "        filtered_query = []\n",
    "\n",
    "        query = query.rstrip(\"\\n\")\n",
    "        # Tokenizing\n",
    "        tokens = tokenizer.tokenize(query.lower())\n",
    "\n",
    "        # Stop-Word Removal + Lemmatization\n",
    "        for token in tokens:\n",
    "            if(token not in stop_words):\n",
    "                filtered_query.append(lemmatizer.lemmatize(token))\n",
    "        tf_idf_score = tf_idf(filtered_query,inv_pos_index)\n",
    "        champ_local_score = champ(filtered_query, 'local')\n",
    "        champ_global_score = champ(filtered_query, 'global')\n",
    "\n",
    "        cluster_score = cluster_scoring(filtered_query, follow_list, lead_inv_pos_index)\n",
    "        #  Writing query data\n",
    "        text_file.write(query + \"\\n\")\n",
    "        text_file.write(list_to_string(tf_idf_score))\n",
    "        text_file.write(list_to_string(champ_local_score))\n",
    "        text_file.write(list_to_string(champ_global_score))  \n",
    "        text_file.write(list_to_string(cluster_score))  \n",
    "\n",
    "        text_file.write(\"\\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Scoring \n",
    " \n",
    "# Function for finding followers to each leader\n",
    "def find_followers(leaders, lead_inv_pos_index):\n",
    "    follow_dict = {}\n",
    "    for i in range(len(file_list)):\n",
    "        leader_score = []\n",
    "        if(i in leaders): \n",
    "            pass\n",
    "        else:\n",
    "            leader = tf_idf(word_vec[i],lead_inv_pos_index)[0]\n",
    "            if(leader[0] not in follow_dict):\n",
    "                follow_dict[leader[0]] = [i]\n",
    "            else:\n",
    "                follow_dict[leader[0]].append(i)\n",
    "    \n",
    "    return follow_dict\n",
    "\n",
    "# Part-IV Cluster Scoring Method\n",
    "def cluster_scoring(filtered_query, follow_list, lead_inv_pos_index):\n",
    "    score_dict = {}\n",
    "    leader = tf_idf(filtered_query, lead_inv_pos_index)[0]\n",
    "    score_dict[leader[0]] = leader[1]\n",
    "    \n",
    "    followers = follow_list[leader[0]]\n",
    "\n",
    "    # Calculating |Vq|\n",
    "    Vq_norm = 0\n",
    "    for query in filtered_query:\n",
    "        Vq_norm = Vq_norm + inv_pos_index[query][0]\n",
    "    Vq_norm = math.sqrt(Vq_norm)    \n",
    "\n",
    "    # Calculating tf-idf scores for follow_list\n",
    "    for query in filtered_query:\n",
    "            # Taking only the follower documents into consideration\n",
    "            doc_list = [doc for doc in inv_pos_index[query][1:] if doc[0] in followers]\n",
    "            for doc in doc_list:\n",
    "                tf = doc[1]\n",
    "                idf  = inv_pos_index[query][0]\n",
    "                tidf = (idf * (tf*idf))/(Vq_norm * Vd_norm[doc[0]])\n",
    "            \n",
    "                if(doc[0] in score_dict):\n",
    "                    score_dict[doc[0]] = score_dict[doc[0]] + tidf\n",
    "            \n",
    "                else:\n",
    "                    score_dict[doc[0]] = tidf\n",
    "    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return score_dict[:min(10,len(score_dict))]"
   ]
  }
 ]
}