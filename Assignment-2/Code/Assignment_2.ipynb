{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda1c5afe4909b94d1f9ad8012633fc5cb5",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pickle5 as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    " \n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_path = parent_dir + \"/Dataset\"\n",
    "\n",
    "file_list = os.listdir(data_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Adding custom words to the stop-words list\n",
    "cust_stop_words = [\"'s\"]\n",
    "for temp in cust_stop_words:\n",
    "    stop_words.add(temp)\n",
    "\n",
    "global inv_pos_index = {}\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    filtered_tokens = []\n",
    "    txt_file = open(data_path + \"/\" + str(i) + \".html\")\n",
    "    soup = BeautifulSoup(txt_file, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Tokenizing\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Stop-Word Removal + Lemmatization\n",
    "    for token in tokens:\n",
    "        if(token not in stop_words):\n",
    "            filtered_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    # Finding term-freq of each token in the token-list\n",
    "    token_freq = Counter(filtered_tokens)\n",
    "\n",
    "    # Adding the (d,tf) pair to the Inverted Positional Index\n",
    "    for token in token_freq:\n",
    "        tf = math.log10(1+token_freq[token])\n",
    "        if(token not in inv_pos_index):\n",
    "            inv_pos_index[token] = [0,tuple(i,round(tf,5))]\n",
    "        else:\n",
    "            inv_pos_index[token].append(tuple(i,round(tf,5)))\n",
    "\n",
    "\n",
    "# Computing & Storing the idf value for each term\n",
    "for token in inv_pos_index:\n",
    "    df = len(inv_pos_index[token])-1\n",
    "    idf = math.log10(float(len(file_list)/df))\n",
    "    inv_pos_index[token][0] = idf\n",
    "\n",
    "with open(\"Inv_Pos_Index.json\", 'w') as fp:\n",
    "    json.dump(inv_pos_index, fp, sort_keys=True, indent=3)"
   ]
  },
  {
   "source": [
    "## Building Champion Lists\n",
    "\n",
    "- **ChampionListLocal:** Top 50 tf value document list per-term.\n",
    "\n",
    "- **ChampionListGlobal:** Top 50 tf + g(d) value document list per-term. \n",
    "\n",
    "\n",
    "***NOTE:*** g(d) values are taken from StaticQualityScore.pkl"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary Code -------------------------------------------------------------------------------------------------\n",
    "file_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "data_path = parent_dir + \"/Dataset\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"Inv_Pos_Index.json\", 'r') as fp:\n",
    "    inv_pos_index = json.load(fp)\n",
    "\n",
    "file_list = os.listdir(data_path)\n",
    "# Temporary Code --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Champion List Local\n",
    "global champ_list_local = {}\n",
    "for token in inv_pos_index:\n",
    "    temp = inv_pos_index[token][1:]\n",
    "    temp.sort(key = lambda x: x[1],reverse = True)\n",
    "    temp = [tuple(doc) for doc in temp[:min(50,len(temp))]]\n",
    "    champ_list_local[token] = temp\n",
    "\n",
    "\n",
    "# Champion List Global\n",
    "global champ_list_global = {}\n",
    "with open(parent_dir + '/StaticQualityScore.pkl','rb') as fp:\n",
    "    g = pickle.load(fp)\n",
    "\n",
    "    for token in inv_pos_index:\n",
    "        temp = inv_pos_index[token][1:]\n",
    "        temp.sort(key = lambda x: x[1] + g[x[0]],reverse = True)\n",
    "        temp = [tuple(doc)  for doc in temp[:min(50,len(temp))]]        \n",
    "        champ_list_global[token] = temp\n",
    "\n",
    "# Calculating sum of squares\n",
    "Vd_norm = [0]*len(file_list)\n",
    "\n",
    "for token in inv_pos_index:\n",
    "    temp = inv_pos_index[token]\n",
    "    for i in range(1,len(temp)):\n",
    "        doc = temp[i][0]\n",
    "        tf_idf = temp[i][1] * temp[0]\n",
    "        Vd_norm[doc] = Vd_norm[doc] + (tf_idf**2)\n",
    "    \n",
    "#  Calculating sqrt for Vd_norm\n",
    "for val in Vd_norm:\n",
    "    val = math.sqrt(val)"
   ]
  },
  {
   "source": [
    "## Query Processing & Scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"query.txt\",\"r\") as query_file:\n",
    "# with open(sys.argv[1],\"r\") as query_file:  # For taking the commandline input for the query text file\n",
    "    query_list = query_file.readlines()\n",
    "\n",
    "def tf_idf(filtered_query):\n",
    "    score_dict = {}\n",
    "    Vq_norm = 0\n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        Vq_norm = Vq_norm + inv_pos_index[query][0]\n",
    "    Vq_norm = math.sqrt(Vq_norm)    \n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        doc_list = inv_pos_index[query][1:]\n",
    "        for doc in doc_list:\n",
    "            tf = doc[1]\n",
    "            idf  = inv_pos_index[query][0]\n",
    "            tf_idf = (idf * (tf*idf))/(Vq_norm * Vd_norm[doc[0]])\n",
    "        \n",
    "            if(doc[0] in score_dict):\n",
    "                score_dict[doc[0]] = score_dict[doc[0]] + tf_idf\n",
    "        \n",
    "            else:\n",
    "                score_dict[doc[0]] = tf_idf\n",
    "\n",
    "    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return score_dict[:10]\n",
    "\n",
    "def champ(filtered_query, score_type):\n",
    "    if(score_type == 'local'):\n",
    "        champ_dict = champ_list_local\n",
    "    elif(score_type == 'global'):\n",
    "        champ_dict = champ_list_global\n",
    "    else:\n",
    "        print(\"champ_score() wrong call\")\n",
    "        return\n",
    "\n",
    "    score_dict = {}\n",
    "    Vq_norm = 0\n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        Vq_norm = Vq_norm + inv_pos_index[query][0]\n",
    "    Vq_norm = math.sqrt(Vq_norm)    \n",
    "\n",
    "    # Calculating |Vq|\n",
    "    for query in filtered_query:\n",
    "        doc_list = champ_dict[query]\n",
    "        for doc in doc_list:\n",
    "            tf = doc[1]\n",
    "            idf  = inv_pos_index[query][0]\n",
    "            tf_idf = (idf * (tf*idf))/(Vq_norm * Vd_norm[doc[0]])\n",
    "        \n",
    "            if(doc[0] in score_dict):\n",
    "                score_dict[doc[0]] = score_dict[doc[0]] + tf_idf\n",
    "        \n",
    "            else:\n",
    "                score_dict[doc[0]] = tf_idf\n",
    "    score_dict = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return score_dict[:10]\n",
    "\n",
    "def list_to_string(score_list):\n",
    "    text = \"\"\n",
    "    for doc in score_list:\n",
    "        text = text + '<' + str(doc[0]) + ',' +str(round(doc[1],5)) + '>,'\n",
    "\n",
    "    text = text[:-1] + '\\n'\n",
    "    return text\n",
    "\n",
    "\n",
    "with open(\"RESULTS1_18CH3FP07.txt\",\"wt\") as text_file:\n",
    "    for query in query_list:\n",
    "        filtered_query = []\n",
    "\n",
    "        query = query.rstrip(\"\\n\")\n",
    "        # Tokenizing\n",
    "        tokens = tokenizer.tokenize(query.lower())\n",
    "\n",
    "        # Stop-Word Removal + Lemmatization\n",
    "        for token in tokens:\n",
    "            if(token not in stop_words):\n",
    "                filtered_query.append(lemmatizer.lemmatize(token))\n",
    "        tf_idf_score = tf_idf(filtered_query)\n",
    "        champ_local_score = champ(filtered_query, 'local')\n",
    "        champ_global_score = champ(filtered_query, 'global')\n",
    "\n",
    "        #  Writing query data\n",
    "        text_file.write(query + \"\\n\")\n",
    "        text_file.write(list_to_string(tf_idf_score))\n",
    "        text_file.write(list_to_string(champ_local_score))\n",
    "        text_file.write(list_to_string(champ_global_score))  \n",
    "\n",
    "        text_file.write(\"\\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(793, 2.30103), (55, 2.20412), (143, 2.15229), (567, 2.11394), (882, 2.10037), (372, 2.09342), (726, 2.08279), (604, 2.07555), (429, 2.06446), (443, 2.06446), (574, 2.06446), (870, 2.0607), (477, 2.03743), (928, 2.03342), (179, 2.02531), (485, 2.01284), (393, 2.0086), (359, 2.00432), (522, 2.00432), (584, 2.0), (864, 1.99564), (656, 1.99123), (173, 1.98677), (442, 1.98677), (683, 1.98677), (842, 1.98677), (776, 1.98227), (135, 1.97313), (845, 1.97313), (25, 1.96379), (186, 1.96379), (541, 1.96379), (658, 1.96379), (21, 1.95904), (220, 1.95424), (371, 1.95424), (572, 1.95424), (791, 1.95424), (397, 1.94939), (71, 1.93952), (377, 1.93952), (948, 1.93952), (36, 1.9345), (183, 1.9345), (30, 1.92942), (492, 1.92942), (767, 1.92942), (895, 1.92942), (128, 1.92428), (720, 1.92428)]\n"
     ]
    }
   ],
   "source": [
    "print(champ_list_local['year'])"
   ]
  }
 ]
}