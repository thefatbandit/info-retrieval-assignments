{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda1c5afe4909b94d1f9ad8012633fc5cb5",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pickle5 as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    " \n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Count-Vectorizer & TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Naive-Bayes models\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# KNN classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# for F1 scores/classification_reports\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# TO BE TAKEN AS SYS.ARG[1]\n",
    "data_path = os.getcwd() + \"/Dataset\"\n",
    "# =====================================================================================\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Adding custom words to the stop-words list\n",
    "cust_stop_words = [\"'s\"]\n",
    "for temp in cust_stop_words:\n",
    "    stop_words.add(temp)\n",
    "\n",
    "def cleanup_text(text):\n",
    "    \"\"\"\n",
    "    Input: Un-processed text (str)\n",
    "\n",
    "    Output: Processed text (str)\n",
    "    \"\"\"\n",
    "    # Tokenizing\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    filtered_tokens = []\n",
    "    # Stop-Word Removal + Lemmatization\n",
    "    for token in tokens:\n",
    "        if(token not in stop_words):\n",
    "            filtered_tokens.append(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def create_dataset(path):\n",
    "    class1_path = data_path + \"/class1\"\n",
    "    class2_path = data_path + \"/class2\"\n",
    "\n",
    "    class1_train_file_list = os.listdir(class1_path + \"/train\")\n",
    "    class1_test_file_list = os.listdir(class1_path + \"/test\")\n",
    "\n",
    "    class2_train_file_list = os.listdir(class2_path + \"/train\")\n",
    "    class2_test_file_list = os.listdir(class2_path + \"/test\")\n",
    "    \n",
    "\n",
    "    train_lbl= [1]*len(class1_train_file_list) + [2]*len(class2_train_file_list)\n",
    "    test_lbl= [1]*len(class1_test_file_list) + [2]*len(class2_test_file_list)\n",
    "    train_arr, test_arr = [], []\n",
    "\n",
    "    for file in class1_train_file_list:\n",
    "        text = open(class1_path + \"/train/\" + file, 'rb').read().decode(errors='replace')\n",
    "        train_arr.append(cleanup_text(text))\n",
    "    \n",
    "    for file in class2_train_file_list:\n",
    "        text = open(class2_path + \"/train/\" + file, 'rb').read().decode(errors='replace')\n",
    "        train_arr.append(cleanup_text(text))\n",
    "    \n",
    "    for file in class1_test_file_list:\n",
    "        text = open(class1_path + \"/test/\" + file, 'rb').read().decode(errors='replace')\n",
    "        test_arr.append(cleanup_text(text))\n",
    "\n",
    "    for file in class2_test_file_list:\n",
    "        text = open(class2_path + \"/test/\" + file, 'rb').read().decode(errors='replace')\n",
    "        test_arr.append(cleanup_text(text))\n",
    "\n",
    "    return train_arr, train_lbl, test_arr, test_lbl, len(class1_train_file_list)     \n",
    "\n",
    "def feature_selection(x_traintf, y_train, x_testtf, k_best):\n",
    "    selector = SelectKBest(mutual_info_classif, k=k_best)\n",
    "    selector.fit(x_traintf, y_train)\n",
    "    x_train = selector.transform(x_traintf)\n",
    "    x_test = selector.transform(x_testtf)\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def naive_bayes(x_traintf, y_train, x_testtf, y_test):\n",
    "    k_best = [1,10,100,1000,10000]\n",
    "    mnb = MultinomialNB()\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    text = \"Multinomial Naive Bayes f1-scores \\n\"\n",
    "    # Multinomial-NB\n",
    "    for entry in k_best:\n",
    "        x_train, x_test = feature_selection(x_traintf, y_train, x_testtf, entry)\n",
    "        mnb.fit(x_train.toarray(), y_train)\n",
    "        yhat = mnb.predict(x_test.toarray())\n",
    "        f1 = f1_score(y_test, yhat, average='micro')\n",
    "        text = text + \"Top \" + str(entry) + \"-features = \" + str(round(f1,5)) + \"\\n\"\n",
    "    \n",
    "    text = text + \"\\n\" + \"Bernoulli Naive Bayes f1-scores \\n\"\n",
    "    # Gaussian-NB\n",
    "    for entry in k_best:\n",
    "        x_train, x_test = feature_selection(x_traintf, y_train, x_testtf, entry)\n",
    "        bnb.fit(x_train, y_train)\n",
    "        yhat = bnb.predict(x_test)\n",
    "        f1 = f1_score(y_test, yhat, average='micro')\n",
    "        text = text + \"Top \" + str(entry) + \"-features = \" + str(round(f1,5)) + \"\\n\"\n",
    "    text = text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "    \n",
    "def rocchio(x_train, y_train, x_test, y_test, n_class1):\n",
    "    b = [0,0.01,0.05,0.1]\n",
    "    centroid = np.zeros((2,x_train.shape[1]))\n",
    "    centroid[0] = np.sum(x_train[:n_class1], axis=0)/n_class1\n",
    "    centroid[1] = np.sum(x_train[n_class1:], axis=0)/(len(y_train)-n_class1)\n",
    "    \n",
    "    text = \"Rocchio Classifier f1-scores \\n\"\n",
    "    for entry in b:\n",
    "        yhat = np.zeros((x_test.shape[0],1))\n",
    "        for i in range(x_test.shape[0]):\n",
    "            dist_1 = np.linalg.norm(centroid[0] - x_test[i])\n",
    "            dist_2 = np.linalg.norm(centroid[1] - x_test[i])\n",
    "            if(dist_1 < (dist_2 - entry)):\n",
    "                yhat[i] = 1\n",
    "            elif(dist_2 < (dist_1 - entry)):\n",
    "                yhat[i] = 2\n",
    "            else:\n",
    "                yhat[i] = 0\n",
    "        f1 = f1_score(y_test, yhat, average='micro')\n",
    "        text = text + \"b-\" + str(entry) + \" = \" + str(round(f1,5)) + \"\\n\"\n",
    "    text = text + \"\\n\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def knn(x_train, y_train, x_test, y_test):\n",
    "    nn = [1,10,50]\n",
    "    text = \"KNN Classifier f1-scores \\n\"\n",
    "    for entry in nn:\n",
    "        knn = KNeighborsClassifier(n_neighbors=entry)\n",
    "        knn.fit(x_train, y_train)\n",
    "        yhat = knn.predict(x_test)\n",
    "        f1 = f1_score(y_test, yhat, average='micro')\n",
    "        text = text + str(entry) + \"-neighbours = \" + str(round(f1,5)) + \"\\n\"\n",
    "\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming train/test dataset for each class\n",
    "train_arr, train_lbl, test_arr, test_lbl, n_class1 = create_dataset(data_path)\n",
    "\n",
    "# Count-Vectorization\n",
    "vectorizer.fit(train_arr)\n",
    "train_mat = vectorizer.transform(train_arr)\n",
    "test_mat = vectorizer.transform(test_arr)\n",
    "\n",
    "# Tf-idf Transformer\n",
    "tfidf.fit(train_mat)\n",
    "train_tfmat = tfidf.transform(train_mat)\n",
    "test_tfmat = tfidf.transform(test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(naive_bayes(train_mat, train_lbl, test_mat, test_lbl))\n",
    "print(rocchio(train_tfmat.toarray(), train_lbl, test_tfmat.toarray(), test_lbl, n_class1))\n",
    "# print(knn(train_tfmat, train_lbl, test_tfmat, test_lbl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}